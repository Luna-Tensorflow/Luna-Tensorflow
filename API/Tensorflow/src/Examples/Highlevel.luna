import Tensorflow.Types
import Tensorflow.Tensor
import Tensorflow.Operations

import Tensorflow.Layers.Input
import Tensorflow.Layers.Dense
import Tensorflow.Optimizers.GradientDescent
import Tensorflow.Losses.MSE
import Tensorflow.Model

def generateTest n:
    tx = (1 . upto n) . each (i:
        t = randomTensor FloatType [2, 1] 2.0.negate 2.0
        l = t . toList
        x = l . at 0 . get
        y = l . at 1 . get
        nt = tensorFromList FloatType [4,1] [x, x*x, y, y*y]
        nt)

    ty = tx . each(t:
        xx = t . at [1, 0]
        yy = t . at [3, 0] # z = t . at [2, 0]
        w = xx + yy # + z*z
        incircle = if w <= 1.0 then 1.0 else 0.0
        y = tensorFromList1d FloatType [incircle]
        y)

    (tx, ty)

def absPatch x:
    if x < 0.0 then x.negate else x # TODO remove after new Luna version is released

def exampleNet:
    n = 10000
    print "Loading data"

    (x,y) = generateTest n
    (testX, testY) = generateTest n

    print "Building net"
    i = input FloatType [4, 1]
    d1 = denseWithActivation 2 tanh i
    d2 = denseWithActivation 1 sigmoid d1

    print "Compiling model"
    opt = gradientDescentOptimizer 0.1
    loss = meanSquareError

    model = makeModel i d2 opt loss

    print "Training"
    fitted = model.train x y

    print "Evaluation"
    predY = testX . each (tx: fitted.evaluate tx . head . get)

    result = (predY . zip testY) . fold 0.0 ((pY, tY): acc: acc + (absPatch (pY.atIndex 0 - tY.atIndex 0)) . round 0) # 0 is ok, 1 is fail

    maxn = n . toReal
    effic = 100.0 * (maxn - result) / maxn
    print ("Efficiency: " + (effic.toText) + "%")




