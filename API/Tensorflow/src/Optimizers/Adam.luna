import Tensorflow.Variables
import Tensorflow.Operations
import Tensorflow.Types
import Tensorflow.Gradient
import Tensorflow.Graph
import Tensorflow.GeneratedOps
import Tensorflow.Tensor

# Optimizer that implements the Adam algorithm.
class Adam:
      beta1Power :: Real
      beta2Power :: Real
      lr :: Real
      beta1 :: Real
      beta2 :: Real
      epsilon :: Real
      useNesterov :: Bool

      # Returns Adam Optimizer in Text type.
      # `return` :: Text
      # toText :: Text
      def toText:
          "Adam beta1Power=" + self.beta1Power.toText + " beta2Power=" + self.beta2Power.toText + " lr=" + self.lr.toText +
                  " beta1=" + self.lr.toText + " beta2=" + self.beta2.toText + " epsilon=" + self.epsilon.toText

      # Returns Adam Optimizer in JSON type.
      # `return` :: JSON
      # toJSON :: JSON
      def toJSON:
          self.toText.toJSON

      # Function to update variables in TFGraph based on expected output, real output and loss function according to the Adam.
      # Arguments:
      # `yTrue` :: TFOutput a
      # `yPred` :: TFOutput a
      # `loss` :: TFOutput a -> TFOutput a -> TFOutput b
      # `variables` :: List Variable 
      # `return` :: TFGraph
      # makeOptimizingGraph :: TFOutput a -> TFOutput a -> (TFOutput a -> TFOutput a -> TFOutput b) -> List Variable -> TFGraph
      def makeOptimizingGraph yTrue yPred loss variables:
          err = loss yTrue yPred
          grads = Gradients.make [err] variables []
          varsAndGrads = variables.zip grads

          def makeAndApplyAdam ((v, g), i):
              beta1Power = Operations.constFromReal v.typetag self.beta1Power
              beta2Power = Operations.constFromReal v.typetag self.beta2Power
              lr = Operations.constFromReal v.typetag self.lr
              beta1 = Operations.constFromReal v.typetag self.beta1
              beta2 = Operations.constFromReal v.typetag self.beta2
              epsilon = Operations.constFromReal v.typetag self.epsilon

              zerosTensor = Operations.zerosLike v . eval
              m = Variables.make ("m" + i.toText) zerosTensor
              vAdam = Variables.make ("v" + i.toText) zerosTensor
              GeneratedOps.applyAdam "" v m vAdam beta1Power beta2Power lr beta1 beta2 epsilon g v.typetag False self.useNesterov

          errWithSideEffect = Variables.sequence' err $ varsAndGrads.zip (0.upto (variables.length - 1)) . map makeAndApplyAdam

          TFGraphMaker.makeFromOutput errWithSideEffect

# Adam Optimizer constructing and usage.
class AdamOptimizer:
    AdamOptimizer

    # Adam Oprimizer constructor.
    # Arguments:
    # `beta1Power` :: Real
    # `beta2Power` :: Real
    # `lr` :: Real
    # `beta1` :: Real
    # `beta2` :: Real
    # `epsilon` :: Real
    # `useNesterov :: Bool
    # `return` :: Adam
    # create :: Real -> Real -> Real -> Real -> Real -> Bool -> Adam
    def create beta1Power beta2Power lr beta1 beta2 epsilon useNesterov:
        Adam beta1Power beta2Power lr beta1 beta2 epsilon useNesterov
